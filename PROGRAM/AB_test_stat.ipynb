{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_abtest_data(N_control, N_experiment, p_control, p_experiment, \n",
    "                         control_label='Control',test_label='Experiment',random_size=True):\n",
    "    \"\"\"Returns a pandas dataframe with fake CTR data\n",
    "    Example:\n",
    "    Parameters:\n",
    "        N_control (int): sample size for control group\n",
    "        N_experiment (int): sample size for test group\n",
    "        p_control (float): conversion rate of control group\n",
    "        p_experiment (float): conversion rate of test group\n",
    "        random_size(boolean): Decide whether the experiment/control group size is drawn randomly\n",
    "        control_label (str)\n",
    "        test_label (str)\n",
    "    Returns:\n",
    "        df (df)\n",
    "    \"\"\"\n",
    "    # initiate empty container\n",
    "    data = []\n",
    "    # total amount of rows in the data\n",
    "    N = N_control + N_experiment\n",
    "    # distribute events based on proportion of group size\n",
    "    group_bern = stats.bernoulli(N_control / (N_control + N_experiment))\n",
    "    # initiate bernoulli distributions from which to randomly sample\n",
    "    control_bern = stats.bernoulli(p_control)\n",
    "    experiment_bern = stats.bernoulli(p_experiment)\n",
    "    if random_size:\n",
    "        for idx in range(N):\n",
    "            # initite empty row\n",
    "            row = {}\n",
    "            # for 'ts' column\n",
    "            # assign group based on 50/50 probability\n",
    "            row['group'] = group_bern.rvs()\n",
    "            if row['group'] == 0:\n",
    "                # assign conversion based on provided parameters\n",
    "                row['converted'] = control_bern.rvs()\n",
    "            else:\n",
    "                row['converted'] = experiment_bern.rvs()\n",
    "            # collect row into data container\n",
    "            data.append(row)    \n",
    "    else:\n",
    "        for idx in range(N_control):\n",
    "            # initite empty row\n",
    "            row = {}\n",
    "            row['group'] = 0\n",
    "            row['converted']=control_bern.rvs()\n",
    "            data.append(row)\n",
    "        for idx in range(N_experiment):\n",
    "            # initite empty row\n",
    "            row = {}\n",
    "            row['group'] = 1\n",
    "            row['converted']=experiment_bern.rvs()\n",
    "            data.append(row)\n",
    "    # convert data into pandas dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # transform group labels of 0s and 1s to user-defined group labels\n",
    "    df['group'] = df['group'].apply(\n",
    "        lambda x: control_label if x == 0 else test_label)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooled_prob(N_A, N_B, X_A, X_B):\n",
    "    \"\"\"Returns pooled probability for two samples\"\"\"\n",
    "    return (X_A + X_B) / (N_A + N_B)\n",
    "\n",
    "def pooled_SE(N_A, N_B, X_A, X_B):\n",
    "    \"\"\"Returns the pooled standard error for two samples\"\"\"\n",
    "    p_hat = pooled_prob(N_A, N_B, X_A, X_B)\n",
    "    SE = np.sqrt(p_hat * (1 - p_hat) * (1 / N_A + 1 / N_B))\n",
    "    return SE\n",
    "\n",
    "def confidence_interval(sample_mean=0, sample_std=1, sample_size=1,\n",
    "                        sig_level=0.05):\n",
    "    \"\"\"Returns the confidence interval as a tuple\"\"\"\n",
    "    z = z_val(sig_level)\n",
    "    left = sample_mean - z * sample_std / np.sqrt(sample_size)\n",
    "    right = sample_mean + z * sample_std / np.sqrt(sample_size)\n",
    "    return (left, right)\n",
    "\n",
    "def z_val(sig_level=0.05, two_tailed=True):\n",
    "    \"\"\"Returns the z value for a given significance level\"\"\"\n",
    "    z_dist = stats.norm(0,1)\n",
    "    if two_tailed:\n",
    "        sig_level = sig_level/2\n",
    "        area = 1 - sig_level\n",
    "    else:\n",
    "        area = 1 - sig_level\n",
    "    z = z_dist.ppf(area)\n",
    "    return z\n",
    "\n",
    "def p_val(N_A, N_B, p_A, p_B):\n",
    "    \"\"\"Returns the p-value for an A/B test\"\"\"\n",
    "    return stats.binom(N_A, p_A).pmf(p_B * N_B)\n",
    "\n",
    "def ab_dist(stderr, d_hat=0, group_type='control'):\n",
    "    \"\"\"Returns a distribution object depending on group type\n",
    "    Examples:\n",
    "    Parameters:\n",
    "        stderr (float): pooled standard error of two independent samples\n",
    "        d_hat (float): the mean difference between two independent samples\n",
    "        group_type (string): 'control' and 'test' are supported\n",
    "    Returns:\n",
    "        dist (scipy.stats distribution object)\n",
    "    \"\"\"\n",
    "    if group_type == 'control':\n",
    "        sample_mean = 0\n",
    "    elif group_type == 'test':\n",
    "        sample_mean = d_hat\n",
    "    # create a normal distribution which is dependent on mean and std dev\n",
    "    dist = stats.norm(sample_mean, stderr)\n",
    "    return dist\n",
    "\n",
    "def min_sample_size(bcr, mde, power=0.8, sig_level=0.05):\n",
    "    \"\"\"Returns the minimum sample size to set up a split test\n",
    "    Arguments:\n",
    "        bcr (float): probability of success for control, sometimes\n",
    "        referred to as baseline conversion rate\n",
    "        mde (float): minimum change in measurement between control\n",
    "        group and test group if alternative hypothesis is true, sometimes\n",
    "        referred to as minimum detectable effect\n",
    "        power (float): probability of rejecting the null hypothesis when the\n",
    "        null hypothesis is false, typically 0.8\n",
    "        sig_level (float): significance level often denoted as alpha,\n",
    "        typically 0.05\n",
    "    Returns:\n",
    "        min_N: minimum sample size\n",
    "    \"\"\"\n",
    "    # standard normal distribution to determine z-values\n",
    "    standard_norm = stats.norm(0,1)\n",
    "    # find Z_beta from desired power\n",
    "    Z_beta = standard_norm.ppf(power)\n",
    "    # find Z_alpha\n",
    "    Z_alpha = standard_norm.ppf(1-sig_level/2)\n",
    "    # Calculate minimum sample size\n",
    "    min_N = (Z_alpha*np.sqrt(2*bcr*(1-bcr))+Z_beta*np.sqrt(bcr*(1-bcr)+(bcr+mde)*(1-bcr-mde)))**2 / mde**2\n",
    "    return min_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3622.6163992757115"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_sample_size(0.1,0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
